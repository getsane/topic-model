{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model - LDA using Collapsed Gibbs Sampling\n",
    "Sandeep Shetty  \n",
    "12/10/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation (LDA)** is a probabilistic generative topic modeling approach (Blei et al, 2002). Given a distribution of words contained in a document or set of documents, LDA infers the latent (or hidden) distribution of topics within a document and/or a corpus of documents. \n",
    "\n",
    "In LDA, one sets up a generative model of document formation, where each document is a mixture of topics, and each topic is a distribution over words. Using Bayesian principles, LDA allows one to infer the posterior distribution of topic given the observed document/corpus, and Bayesian networks to exploit the conditional dependencies in the hierarchical model to make the computation of the posterior distribution feasible. \n",
    "\n",
    "Given the complex nature of the posterior distribution, the estimation is achieved through simulation procedures. There are several different approaches  in the literature, each with its pros and cons. Blei et al (2002) use a variational EM algorithm (VI). VI is fast but not accurate.  Griffith & Steyvers (2005) use MCMC-technique of Collapsed Gibbs Sampling (CGS). CGS is \"more\" precise and popular among topic model researchers but it is inefficient on large samples (Blei et al 2018).\n",
    "\n",
    "In this exercise, I used **Collapsed Gibbs Sampling (CGS)** to approximate the posterior distribution of hidden variables, i.e. topic assignment. This distribution is subsequently used to estimate the key parameters of interest - the distribution of topics within each documnent in the corpus and the distribution of topics over vocabulory. The number of topics to search in a corpus is an provided by the user. \n",
    "\n",
    "The posterior distribution of latent variable, i.e. topic assignment, that is approximated using CGS is given as:  \n",
    "  \n",
    "$${\n",
    "P( z_i = j \\text{ }| \\text{ } z_{-i}, w_i, m_i ) \n",
    "    \\propto \\frac{ n^{(t)}_{k,-i} + \\beta }{ \\sum^V_{ t = 1 }n^{(t)}_{k,-i} + V\\beta } \\times\n",
    "      \\frac{ n^{(t)}_{m,-i} + \\alpha }{ \\sum^K_{ k = 1 }n^{(k)}_{m} + K\\alpha - 1 }}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$P( z_i = j \\text{ }| \\text{ } z_{-i}, w_i, m_i ) $ : The probability that token $i$ in document $d$ is assigned to topic $j$ conditioned (or knowing) the topic assignments of all other tokens in document $d$ and the observed word $w$ at token $i$     \n",
    "$z_{âˆ’i}$ : Topic assignments of all other tokens;     \n",
    "$w_{i}$ : Word (index) of the ith token;      \n",
    "$m_{i}$ : Document containing the ith token.  \n",
    "\n",
    "This probability is proportional to the right side:-  \n",
    "$V$: Total number of words in the corpus   \n",
    "$k$: one of the topics, $K$ is total number of topics   \n",
    "$n^{(t)}_{k,-i}$ : number of words, excluding $i$, which are assigned topic $k$ in document $m$   \n",
    "$n^{(t)}_{m,-i}$ : number of words, excluding $i$, in document $m$ those are assigned topic $k$  \n",
    "$\\sum^V_{ t = 1 }n^{(t)}_{k,-i}$: number of words,excluding $i$, are assigned topic $k$  \n",
    "$\\sum^K_{ k = 1 }n^{(k)}_{m}$: length of document $m$  \n",
    "$\\beta$ : Parameter that sets the topic distribution for the words.  \n",
    "$\\alpha$ : Parameter that sets the topic distribution for the documents.  \n",
    "\n",
    "The right hand side is basically a product of two proportions, which balances the within document occurrence of a given topic with that observed and across documents in a corpus. \n",
    "\n",
    "The parameters that are computed once the full conditional distribution of topic assignment is obtained via CGS are:\n",
    "\n",
    "$$\\phi_{k,t} = \\frac{n^{(t)}_{k} + \\beta}{\\sum^V_{ t = 1 }n^{(t}_{k} + V\\beta}$$\n",
    "\n",
    "$$\\theta_{m,k} = \\frac{n^{k}_{m} + \\alpha}{\\sum^K_{ k = 1 }n^{(k)}_{m} + K\\alpha}$$\n",
    "\n",
    "$\\phi_{k,t}$ is the probability of word $t$ for topic $k$, and $\\theta_{m,k}$ is the proportion of topic $k$ in document $m$. \n",
    "\n",
    "Since CGS is quite popular among the topic model research community, in the literature there are several alterations proposed to CGS to improve its performance on large sample. However, the issue that gathers very scant mention in the topic modeling community but of importance to many applied Bayesian researchers is the discussion of convergence of CGS - whether the starting distribution converges to the target distribution. Although, there are quite a few in the literature on MCMC but these methods rarely agree with each other (Cowles & Carlin (1996). In the topic modelling work, I noticed the use a different measure to assess convergence, than called `Perplexity`, which is basically measuring on how \"well\" the model is clustering semantically related terms in a corpus. \n",
    "\n",
    "In the code below, I have not included any convergence diagnostics to test if the simulated posterior is reaching the target distribution. But I do account for multiple starting points of the distribution in the code. This practice helps to traverse the distribution from different vantage points. However, I do plan to revisit the interesting issue of MCMC convergence disgnostics in the near future.  \n",
    "\n",
    "The LDA model used is as described in Blei et al (2002). There are several other extensions of the LDA and other  topic model tools as described in Minh's topic model protocol document that would be interesting to explore and build from here.\n",
    "\n",
    "Since the code is using the collapsed Gibbs Sampler to simulate, it can be adapted to a parallelized environment for faster computation. \n",
    "\n",
    "**Next steps**: \n",
    "- Parallelization to improve efficiency\n",
    "- Reporting and visualization of output\n",
    "- Writing a more modular (cleaner) code\n",
    "- Research alternate preprocessing techniques\n",
    "- Implement other extended LDA models\n",
    "- Implement Variational Inference technique for parameter estimation (Blei et al)\n",
    "- Matrix-Decomposition methods for Topic Modeling (Minh protocol)\n",
    "- Investigate and implement  convergence diagnotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer # remove stop words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/getsane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = pd.read_csv(\"tag-data.csv\", header=[1])\n",
    "doc = doc.drop(index=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = doc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docli = doc.comments.tolist() #Each item in this list is a separate comment/document\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "A. Remove Stop words  \n",
    "B. Create dictionary of document composed of terms and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkn_doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,k in enumerate(docli):\n",
    "    wrlist = tokenizer.tokenize(k.lower()) \n",
    "    tkn_doc.append([k for k in wrlist if not k in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doct = dict()\n",
    "for i,k in enumerate(tkn_doc):\n",
    "    cnt = dict() \n",
    "    for w in k:\n",
    "        if w in cnt.keys():\n",
    "            cnt[w] += 1\n",
    "        else:\n",
    "            cnt[w] = 1\n",
    "    doct[i] = cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a term-document matrix\n",
    "A. Create a list of terms across the whole corpus  \n",
    "B. Remove alphanumeric terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a sparse matrix of term-document\n",
    "terms = []\n",
    "for i in range(len(doct)):\n",
    "    for j in doct[i].keys():\n",
    "        terms.append(j)\n",
    "        \n",
    "terms = list(set(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove digits and digit combined words\n",
    "\n",
    "p = re.compile(r'\\d+')\n",
    "termClean = [terms[i] for i in range(len(terms)) \n",
    "             if p.findall(terms[i])==[]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain document count and term count\n",
    "len_t = len(termClean)\n",
    "len_d = len(doct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_doc = np.empty((len_t,len_d), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Term-Document Frequency Matrix \n",
    "# Columns-->Document; Rows --> Terms\n",
    "# E.g. term_doc[:,0] -- Extracting 1st document\n",
    "\n",
    "for i in range(len_d):\n",
    "    for t,word in enumerate(termClean):\n",
    "        if word in doct[i].keys():\n",
    "            term_doc[t,i]=doct[i].get(word)\n",
    "        else:\n",
    "            term_doc[t,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Think of using Scipy's COO later to save memory for large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsed Gibbs Sampling \n",
    "A. Generate conditional topic assignment distribution $P( z_i = j \\text{ }| \\text{ } z_{-i}, w_i, d_i ) $  \n",
    "B. Multiple iterations from different starting points of the starting distributions  \n",
    "C. Remove burn-in sample from each different phase\n",
    "D. Collect the different phases and sample to calculate the final parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization of CGS\n",
    "\n",
    "def cgsIntialization(k, wordDocMat):\n",
    "    \n",
    "    '''\n",
    "    Input: \n",
    "        k = # of topic (int)\n",
    "        chain = # of different starting point for the iterations (int)\n",
    "        wordDocMat = Word-document matrix -- [V X d] array\n",
    "    \n",
    "    Output:\n",
    "        n_t_k - number of times term 't' in topic 'k' - [V X k] array\n",
    "        n_k_m - number of times topic 'k' in document 'd' - [d X k] array\n",
    "        n_k - number of words in each topic\n",
    "        n_m - length of document\n",
    "        k - number of topics \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    len_t = wordDocMat.shape[0]  # number of words\n",
    "    len_d = wordDocMat.shape[1]  # number of docs\n",
    "    mat_V_k = np.empty((len_d,len_t,k), dtype = np.int)  #shape==>(doc, words, topic)\n",
    "    \n",
    "    for doc in range(len_d):\n",
    "        for index, val in enumerate(wordDocMat[:,doc]):\n",
    "            if  val!= 0:\n",
    "                mat_V_k[doc,index,:] = np.random.multinomial(1,[1./k]*k) \n",
    "            else:\n",
    "                mat_V_k[doc,index,:] = 0    \n",
    "    return(mat_V_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cgsIteration(wordDocMat, niter = 1200 , burn = 200, k = 5, \n",
    "                 alpha = 0, beta = 0):\n",
    "    \n",
    "    '''\n",
    "    Defintion: Iteration of the CGS \n",
    "    \n",
    "    Input: \n",
    "    \n",
    "        Output returned from cgsIntialization\n",
    "        niter - total number of iterations\n",
    "        burn -  number of iterations to discard\n",
    "        alpha - hyperparameter influencing the shape of the Dirichlet priors\n",
    "        beta -  \" \n",
    "        wordDocMat - term-document matrix (V x d)\n",
    "    \n",
    "    Output: \n",
    "    \n",
    "        finalThetaMK - array [d X k] - distribution of topic in a document\n",
    "                                       (averaged over multiple iterations)\n",
    "        finalphiTK -  array [t X k ] - distribution over word for each topic\n",
    "                                       (averaged over multiple iterations) \n",
    "        mat_V_k - [d X t X k] - topic assignment for each for all document\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    intIter = 0\n",
    "    avgSmp = niter-burn\n",
    "\n",
    "    mat_V_k = cgsIntialization(k, wordDocMat)\n",
    "     \n",
    "    n_k_m = np.sum(mat_V_k, axis=1)  #{doc, topics}\n",
    "    n_t_k = np.sum(mat_V_k, axis=0)  #{words,topics}\n",
    "    n_k = np.sum(n_t_k, axis=0)      #{words}\n",
    "    n_m = np.sum(n_k_m, axis=1)      #{topics}\n",
    "    \n",
    "    lenM = mat_V_k.shape[0]\n",
    "    lenT = mat_V_k.shape[1]\n",
    "     \n",
    "    thetaMK = np.empty((lenM,k,avgSmp), dtype = np.float) \n",
    "    phiTK = np.empty((lenT,k,avgSmp), dtype = np.float)\n",
    "     \n",
    "    while intIter < niter:\n",
    "        \n",
    "        for doc in range(len_d):   # each document\n",
    "            for index, val in enumerate(wordDocMat[:,doc]): # each word\n",
    "                if  val != 0:   \n",
    "                    \n",
    "                    # check if word is in ...\n",
    "                    # subtracting its count from the ...\n",
    "                    # topic assigned to the word in initialization;\n",
    "                    \n",
    "                    n_k_m[doc,:] = n_k_m[doc,:] - mat_V_k[doc,index,:]\n",
    "                    n_t_k[index,:] = n_t_k[index,:]-mat_V_k[doc,index,:]\n",
    "                    n_k = n_k - mat_V_k[doc,index,:]\n",
    "                    n_m[doc] = n_m[doc] - 1 \n",
    "\n",
    "                    prob = np.empty(k, dtype=np.float)\n",
    "\n",
    "                    for topic in range(k):\n",
    "                        rhst1 = (n_k_m[doc,topic] + alpha)/((n_m[doc] + k*alpha)-1) \n",
    "                        rhst2 = (n_t_k[index,topic] + beta)/(n_k[topic] + lenT*beta)\n",
    "\n",
    "                        prob[topic] = rhst1*rhst2\n",
    "\n",
    "                    prob_new = prob/(np.sum(prob))\n",
    "\n",
    "                    mat_V_k[doc,index,:] = np.random.multinomial(1,prob_new) \n",
    "\n",
    "                    n_k_m[doc,:] = n_k_m[doc,:] + mat_V_k[doc,index,:]                                                    \n",
    "                    n_t_k[index,:] = n_t_k[index,:] + mat_V_k[doc,index,:]\n",
    "                    n_k = n_k + mat_V_k[doc,index,:]\n",
    "                    n_m[doc] = n_m[doc] + 1\n",
    "        \n",
    "        #print(intIter)\n",
    "        \n",
    "        # calculate the theta & phi parameters     \n",
    "        if intIter >= burn:\n",
    "            n_k_m1 = np.sum(mat_V_k, axis=1)  #{doc, topics}\n",
    "            print(intIter)\n",
    "            n_t_k1 = np.sum(mat_V_k, axis=0)  #{words,topics}\n",
    "            n_k1 = np.sum(n_t_k, axis=0)      # words count by topic\n",
    "            n_m1 = np.sum(n_k_m, axis=1)      # length of document\n",
    "            \n",
    "            dim3 = intIter - burn\n",
    "            \n",
    "            for it1 in range(lenM):\n",
    "                thetaMK[it1,:, dim3] = (n_k_m1[it1,:] + alpha)/(n_k_m1[it1,:].sum(axis=0) + k*alpha)\n",
    "        \n",
    "            for it2 in range(k):\n",
    "                phiTK[:,it2, dim3] =  (n_t_k1[:,it2] + beta)/(n_t_k1[:,it2].sum(axis=0) + lenT*beta)\n",
    "                \n",
    "        intIter += 1\n",
    "        \n",
    "    finalThetaMK = np.mean(thetaMK, axis=2)\n",
    "    finalphiTK = np.mean(phiTK, axis=2)\n",
    "            \n",
    "    return((finalThetaMK,finalphiTK,mat_V_k))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the LDA-CGS execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execCGS(chain):\n",
    "    '''\n",
    "    Input:\n",
    "    \n",
    "    chain - number of different starting point \n",
    "            covering the starting distributions. \n",
    "            Recommended practise in CGS sampling.\n",
    "    Output:\n",
    "    \n",
    "    Output from cgsIteration\n",
    "    \n",
    "    finalThetaMK - array [d X k] - distribution of topic in a document\n",
    "                                  (averaged over multiple iterations within one chain)\n",
    "    finalphiTK -  array [t X k ] - distribution over word for each topic\n",
    "                                  (averaged over multiple iterations) \n",
    "    mat_V_k - [d X t X k] - topic assignment for each for all document\n",
    "    \n",
    "    '''\n",
    "    chainIter = {} #to store output from each chain separately\n",
    "    \n",
    "    for i in range(chain):\n",
    "        chainIter[i] = cgsIteration(niter = 4, \n",
    "                                    burn = 2, \n",
    "                                    k = 5, \n",
    "                                    alpha = 50/5, #(=50/k)\n",
    "                                    beta = 0.01, \n",
    "                                    wordDocMat=term_doc)\n",
    "    \n",
    "    return(chainIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the CGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chain_out = execCGS(chain=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Topic0\n",
      "0.006163         rin\n",
      "0.006121      earned\n",
      "0.006055       labor\n",
      "0.005914      server\n",
      "0.005811          go\n",
      "0.005807        urge\n",
      "0.005403     minimum\n",
      "0.005335  regulation\n",
      "0.005236         way\n",
      "0.005139  restaurant\n",
      "------------------\n",
      "            Topic1\n",
      "0.005677  proposed\n",
      "0.005623  industry\n",
      "0.005351       pay\n",
      "0.005299       one\n",
      "0.005244      hour\n",
      "0.004980   servers\n",
      "0.004869   minimum\n",
      "0.004869      well\n",
      "0.004652      fair\n",
      "0.004596      paid\n",
      "------------------\n",
      "               Topic2\n",
      "0.006411        staff\n",
      "0.006029    employees\n",
      "0.005920       owners\n",
      "0.005920          pay\n",
      "0.005813       server\n",
      "0.005652  restaurants\n",
      "0.005538          get\n",
      "0.005433         rule\n",
      "0.005378         back\n",
      "0.005328         make\n",
      "------------------\n",
      "           Topic3\n",
      "0.006225     many\n",
      "0.006173     give\n",
      "0.005856  workers\n",
      "0.005751    house\n",
      "0.005751     know\n",
      "0.005751    would\n",
      "0.005488    staff\n",
      "0.005488      pay\n",
      "0.005330  receive\n",
      "0.005277     food\n",
      "------------------\n",
      "              Topic4\n",
      "0.006502       money\n",
      "0.005946        wage\n",
      "0.005783        take\n",
      "0.005394  restaurant\n",
      "0.005340        keep\n",
      "0.005231     working\n",
      "0.005176         get\n",
      "0.005173        away\n",
      "0.005063   employers\n",
      "0.004839        good\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    strin1 = 'Topic'+str(i)\n",
    "    data= pd.DataFrame(np.array(termClean),chain_out[0][1][:,i], columns=[strin1])\n",
    "    print(data.sort_index(ascending=False).iloc[10:20])\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *`{Side note}` *\n",
    "**N-dimensional array**  \n",
    "- A is a 3-D array (two stacked 2D arrays)\n",
    "    - shape of A [documents,words,topics]\n",
    "    - For e.g. shape [1000,5906,5]\n",
    "    - Documents (1000), words (5906), topics (5)\n",
    "        - E.g. 3-D array:-  \n",
    "```c = np.array( [[[  0,  1,  2], [ 10, 12, 13]],\n",
    "    ... [[100,101,102], [110,112,113]]])```\n",
    "                              \n",
    "    \n",
    "- Sum along axis = 0 --> integrating over documents --> results in an array (word, topic) {summed over all documents}\n",
    "- Sum along axis = 1 --> integrating over words --> resulting array is (doc, topic) {summed over all words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
